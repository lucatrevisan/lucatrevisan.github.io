<html>
    
    <head>
        <link rel="stylesheet" href="../luca.css" type="text/css">
            
            <title>Fall 2019 | 40391 Topics in computer science and optimization </title>
            
            
    </head>
    
    <body><center>
        <h1>Fall 2019 &mdash; 40391 Topics in computer science and optimization</h1>
        </center>
    
        This course is an introduction to algorithms for combinatorial optimization problems and for convex optimization problems. Topics will include greedy and dynamic programming approaches to network optimization problems, cut, flow and matching algorithms,  linear programming, gradient descent, mirror descent, and Follow-the-Regularized-Leader algorithm for online convex optimization.
    
    
    <hr>
    
    <h3>General information</h3>
    
    Instructor: <a href=https://lucatrevisan.github.io>Luca Trevisan</a>
    
    <p>
    
    Lectures:
    <ul>
        <li>Tuesdays and Wednesdays 16:15-17:45</li>
    </ul>
    
    <p>
    
    Office hours: Thursdays 11-noon in 3-E1-14 Roentgen (from Sept 12 to Dec 5)
    
    <p>
    
    References:
    <ul>
        <li>A good reference for optimization techniques in discrete algorithms (greedy, dynamic programming, iterated improvements) and a general introduction to graphs and graph algorithms is
            <ul>
                <li>S. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani. Algorithms</li></ul>
            </li>
        
        <li>Dasgupta et al. is also a good reference for max flow, min cut, and linear programming. Additional material on such topics, including online optimization, is in
            <ul>
                <li><a href=https://lucatrevisan.github.io/books/cs261.pdf>My note for Stanford CS261</a></li></ul>
        </li>
        <li>I will write additional notes on online convex optimization and FTRL algorithms.</li>
    
    </ul>
    
    Exam: the exam will be a take-home final
    
    <hr>

    <h3>Past lectures</h3>
    
    <ol>
        <li> 9/11. Overview of the course. Greedy methodology. Minimum spanning tree. (Reference: Dasgupta section 5.1)
        </li>
        
        <p>
        
        <li> 9/17. Dynamic programming methodology. O(nm) time algorithm to compute edit distance. (Reference: Dasgupta 6.3 - it is a good idea to read sections 6.1 and 6.2 as well)
        </li>
              
               <p>
               
        <li> 9/18. Dynamic programming algorithms for shortest paths in the presence of negative weights: single source in time O(|V|*|E|), all pairs in time O(|V|^3), in DAGs in time O(|V| +|E|). (References: Dasgupta sections 4.6, 4.7 and 6.6)
            </li>
              
               <p>
               
        <li> 9/24. Dijsktra's algorithm. (Reference: Dasgupta section 4.4)
            </li>
              
               <p>
               
        
        <li> 9/25. Ford-Fulkerson algorithm for maximum flow. (Reference: lecture 9 of my  <a href=https://lucatrevisan.github.io/books/cs261.pdf>CS261 notes</a> and Section 7.2 of Dasgupta)
            </li>
        
        <p>
        
        <li> 10/1. Analysis of Ford-Fulkerson algorithm, max flow - min cut theorem, analysis of Edmonds-Karp algorithm. (Reference: references of last lecture and section 11.2 of my <a href=https://lucatrevisan.github.io/books/cs261.pdf>CS261 notes</a>)</li>
        
        <p>
        
        <li>10/2. Reduction of matching in bipartite graphs to max flow. (Reference: Dasgupta section 7.3 and sections 14.1 and 14.2 of my <a href=https://lucatrevisan.github.io/books/cs261.pdf>CS261 notes</a>)</li>
        
        <p>
        
        </ol>
    
    <hr>
    
    <h3>Plan for the course</h3>
    
    <ul>
        <li>Lectures 1-2: introduction, greedy algorithms, dynamic programming
            </li>
        <li> Lectures 3-5: iterative-improvement algorithms, max flow, min cut, matching
            </li>
        <li> Lectures 6-9: linear programming, simplex and duality, application to compressed sensing, application to 2-player 0-sum games</li>
        <li> Lectures 10-12: online convex optimization, multiplicative weights, solving 2-player 0-sum games, FTRL, recovering multiplicative weights and gradient descent as special cases of FTRL, Bregman projection</li>
        </ul>
    
    </body>
    
    
    
</html>
